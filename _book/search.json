[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"book covers practical worked examples introductory quantitative methods can easily apply data set also includes discussion example working. cover descriptive basic inferential statistics, including graphs, frequency distributions, central tendency, dispersion, probability, hypothesis testing, tests mean differences, correlation, simple regression, chi-square tests. book designed facilitate graduate students Educational Psychology develop knowledge understanding various statistical concepts procedures R programming supplement resource.","code":""},{"path":"index.html","id":"general-objectives","chapter":"Welcome!","heading":"General Objectives","text":"book based 3 credit semester course “Quantitative Methods - ” taught University Oklahoma Fall 2022. thoroughly following procedures going examples , learner able toIdentify variables correctly falling different scales measurement.Identify variables correctly falling different scales measurement.Identify appropriate techniques analyzing data presented variables different measurement characteristics.Identify appropriate techniques analyzing data presented variables different measurement characteristics.Set manage data sets containing variables R RStudio.Set manage data sets containing variables R RStudio.Analyze data sets using quantitative techniques using R.Analyze data sets using quantitative techniques using R.Distinguish null alternative (research) hypotheses.Distinguish null alternative (research) hypotheses.Distinguish directional non-directional hypothesis.Distinguish directional non-directional hypothesis.Demonstrate concepts “statistical significance” “effect size”.Demonstrate concepts “statistical significance” “effect size”.Analyze effects sampling (e.g., size, strategies) inferences concerning population estimates.Analyze effects sampling (e.g., size, strategies) inferences concerning population estimates.Interpret results statistical analyses.Interpret results statistical analyses.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"acknolwedgement","chapter":"Preface","heading":"0.1 Acknolwedgement","text":"like sincerely thank professor academic advisor Dr Howard M. Crowson (Statistics Real World, n.d.) (Mike Crowson - YouTube, n.d.) providing wonderful opportunity compile resource materials R along providing ample time weekly consultation support. can also follow insightful lectures basic advanced statistial concepts procedure.also like extend gratitude authors textbook “Introduction Statistical Concepts” Hahs-Vaughn Lomax, 4th edition, providing comprehensive resource guided exploration various statistical concepts methodologies. textbook served invaluable reference, helping gain deeper understanding topics apply practical examples using R programming. example data text references inspired textbook.also appreciate developers maintainers R language, R packages, related tools dedicated work providing powerful accessible platform statistical computing data analysis. particular, like acknowledge creators bookdown(Xie, 2016), rmarkdown(Xie et al., 2020), ggplot2 (Wickham, 2016), psych (2023), lavaan (Rosseel, 2012), packages used throughout guide, instrumental demonstrating various statistical concepts procedures throughout book also useful social science research.","code":""},{"path":"preface.html","id":"conventions-used-in-the-book","chapter":"Preface","heading":"0.2 Conventions Used in the Book","text":"Code chunks presented typical Markdown format , code output :Italic: Indicates new terms, URLs, email addresses, file names, file extensions., key highlights.TipThis icon signifies tip, suggestion, general note.Finally, R version currently using:","code":"\n#This is a code block. \n(\"Hello world\")\n#> [1] \"Hello world\"\nversion\n#>                _                                \n#> platform       x86_64-w64-mingw32               \n#> arch           x86_64                           \n#> os             mingw32                          \n#> crt            ucrt                             \n#> system         x86_64, mingw32                  \n#> status                                          \n#> major          4                                \n#> minor          2.1                              \n#> year           2022                             \n#> month          06                               \n#> day            23                               \n#> svn rev        82513                            \n#> language       R                                \n#> version.string R version 4.2.1 (2022-06-23 ucrt)\n#> nickname       Funny-Looking Kid"},{"path":"preface.html","id":"example-data","chapter":"Preface","heading":"0.3 Example Data","text":"example data used form textbook(Hahs-Vaughn & Lomax, 2020). can downloaded official website textbook github. example data compiled form sources available github.","code":""},{"path":"r-basics.html","id":"r-basics","chapter":"1 R Basics","heading":"1 R Basics","text":"sections covers everything need get run statistical analysis using R. Just like programming language, R also base package Integrated Development Environment. Base package need run R code computer. R Studio IDE developed specifically focusing development R programs packages.","code":""},{"path":"r-basics.html","id":"installing-r-base-package.","chapter":"1 R Basics","heading":"1.1 Installing R base package.","text":"R base package can downloaded official website R. , enter inside website select package operating system, download file install . ensure R successfully installed, able run command prompt terminal using R command. Type q() quit R console.","code":"$ R\n\nR version 4.2.1 (2022-06-23 ucrt) -- \"Funny-Looking Kid\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n> q()"},{"path":"r-basics.html","id":"download-r-studio","chapter":"1 R Basics","heading":"1.1.1 Download R studio","text":"Well, need everything command terminal. R community also fully fledged development environment called R Studio free use user friendly work R. can download R studio .following resources provide overview introduction various components R Studio:RStudio IDE Cheat SheetRStudio IDE Cheat SheetGetting Started RStudioGetting Started RStudioThis tutorial help understand basic overview components R studio.","code":""},{"path":"r-basics.html","id":"r-packages","chapter":"1 R Basics","heading":"1.2 R Packages","text":"R simply statistically programming language, R packages developed R community one key reason robustness, reproducibility flexibility. Many statistics programmers developed 100s packages can run even complex statistics functions single line code. using key packages throughout examples also discuss packages.","code":""},{"path":"r-basics.html","id":"r-console","chapter":"1 R Basics","heading":"1.3 R Console","text":"R Console interactive command-line interface R, can enter commands see output. ’s essential component RStudio, can use various tasks data manipulation, statistical analysis, creating graphics.","code":""},{"path":"r-basics.html","id":"getting-help","chapter":"1 R Basics","heading":"1.4 Getting Help","text":"useful syntax ask help","code":"{Get help for an object, in this case for the –-plot– function. \n?plot  #You can also type: help(plot)\n\n#Search the help pages for anything that has the word \"regression\". \n??regression #You can also type:  help.search(\"regression\")\n\n#Search the word \"age\" in the objects available in the current R session.\n apropos(\"age\")\nhelp(package=car) # View documentation in package ‘car’. You can also type: library(help=\"car“)\nhelp(DataABC) # Access codebook for a dataset called ‘DataABC’ in the package ABC\nargs(log) # Description of the command.}"},{"path":"r-basics.html","id":"r-community-and-resources","chapter":"1 R Basics","heading":"1.5 R Community and Resources","text":"R large community developers supporters. following resources may helpful progress research experiments R:","code":""},{"path":"r-basics.html","id":"documentation-websites","chapter":"1 R Basics","heading":"1.5.1 Documentation / Websites","text":"R DocumentationR DocumentationR-bloggersR-bloggersRStudio CommunityRStudio CommunityStack OverflowStack Overflow","code":""},{"path":"r-basics.html","id":"books","chapter":"1 R Basics","heading":"1.5.2 Books","text":"Field, ., Miles, J., & Field, Z. (2012). Discovering statistics using R. London: Sage Publications.Field, ., Miles, J., & Field, Z. (2012). Discovering statistics using R. London: Sage Publications.Kabacoff, R. . (2015). R action: Data analysis graphics R. Manning Publications Co.Kabacoff, R. . (2015). R action: Data analysis graphics R. Manning Publications Co.Navarro, D. J. (2015). Learning statistics R: tutorial psychology students beginners. University Adelaide.Navarro, D. J. (2015). Learning statistics R: tutorial psychology students beginners. University Adelaide.Wickham, H. (2014). Advanced R. CRC Press.Wickham, H. (2014). Advanced R. CRC Press.Wilke, C. O. (2019). Fundamentals Data Visualization: Primer Making Informative Compelling Figures. O’Reilly Media.Wilke, C. O. (2019). Fundamentals Data Visualization: Primer Making Informative Compelling Figures. O’Reilly Media.","code":""},{"path":"r-basics.html","id":"cheatsheets","chapter":"1 R Basics","heading":"1.5.3 Cheatsheets","text":"RStudio CheatsheetsRStudio CheatsheetsDataCamp CheatsheetsDataCamp Cheatsheets","code":""},{"path":"basic-statistical-concepts.html","id":"basic-statistical-concepts","chapter":"2 Basic Statistical Concepts","heading":"2 Basic Statistical Concepts","text":"","code":""},{"path":"basic-statistical-concepts.html","id":"data-types","chapter":"2 Basic Statistical Concepts","heading":"2.1 Data Types","text":"Data types idea computer science program shares similar nomenclature case statistics. Data broadly classified constant variables terms nature execution analysis statistical program.Constant kind data types changed program analysis. eg, value alpha (alpha) always kept constant.Variables data types changed multiple values program.","code":""},{"path":"basic-statistical-concepts.html","id":"types-of-variable","chapter":"2 Basic Statistical Concepts","heading":"2.2 Types of variable","text":"","code":""},{"path":"basic-statistical-concepts.html","id":"quantitative-variables-continuous-and-discrete","chapter":"2 Basic Statistical Concepts","heading":"2.2.1 Quantitative Variables (Continuous and Discrete):","text":"Continuous Variables: Variables can take value within range, typically measured continuous scale. Example: Height, weight, temperature.Continuous Variables: Variables can take value within range, typically measured continuous scale. Example: Height, weight, temperature.Discrete Variables: Variables can take specific values, usually whole numbers counts. Example: Number students class, number books library.\nQualitative Variables (Nominal Ordinal):Discrete Variables: Variables can take specific values, usually whole numbers counts. Example: Number students class, number books library.Nominal Variables: Variables represent categories without inherent order. Example: Gender (male female), types food (vegetarian non-vegetarian).Nominal Variables: Variables represent categories without inherent order. Example: Gender (male female), types food (vegetarian non-vegetarian).Ordinal Variables: Variables represent categories natural order ranking. Example: Education level (elementary, high school, college), customer satisfaction ratings (poor, average, excellent).\n                   VARIABLES\n                       |\n            +----------+-----------+\n            |                      |\n      Quantitative           Qualitative\n            |                      |\n     +------+-------+      +-------+-------+\n     |              |      |               |\nContinuous    Discrete   Nominal      OrdinalOrdinal Variables: Variables represent categories natural order ranking. Example: Education level (elementary, high school, college), customer satisfaction ratings (poor, average, excellent).","code":"                   VARIABLES\n                       |\n            +----------+-----------+\n            |                      |\n      Quantitative           Qualitative\n            |                      |\n     +------+-------+      +-------+-------+\n     |              |      |               |\nContinuous    Discrete   Nominal      Ordinal"},{"path":"basic-statistical-concepts.html","id":"types-of-scales-of-measurement-of-variables","chapter":"2 Basic Statistical Concepts","heading":"2.3 Types of scales of measurement of variables","text":"Four different types scales measurement presented table .Understanding scales measurement important helps determine appropriate statistical techniques interpretations data.","code":""},{"path":"loading-data-in-r.html","id":"loading-data-in-r","chapter":"3 Loading Data in R","heading":"3 Loading Data in R","text":"Data set can directly imported can entered manually directly R ans save R data file also. Lets see can manually enter save import different data formats R Studio.","code":""},{"path":"loading-data-in-r.html","id":"entering-data-in-r","chapter":"3 Loading Data in R","heading":"3.1 Entering Data in R","text":"can start working R right away entering data R. enter numerical data manually, c (stands ‘column’) command used.Similarly, categorical data can also entered using quotation marks.","code":"  age <- c(45, 23, 36, 29)\n    gpa <- c(\"A+\", \"A\", \"B+\", \"B\")\n  "},{"path":"loading-data-in-r.html","id":"importing-csv-file","chapter":"3 Loading Data in R","heading":"3.2 Importing CSV file","text":"read command function R used read data files. read CSV file, can simply move CSV file working directory load file using read.csv command. need readr package read CSV file., csv1 name assigned CSV file R environment. using variable name whenever want work csv file imported.","code":"\nlibrary (readr)\n#> Warning: package 'readr' was built under R version 4.2.2\n  csv1 <- read.csv(\"exampledata/survey1.csv\")\n  \n #To view the structure\n  str(csv1)\n#> 'data.frame':    100 obs. of  28 variables:\n#>  $ sex         : int  0 0 0 0 0 0 0 0 1 1 ...\n#>  $ height      : num  67 67 67 67 73 73 73 73 70 70 ...\n#>  $ shoesize    : num  9.5 9.5 9.5 9.5 13 13 13 13 9 9 ...\n#>  $ smoker      : int  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ handed      : int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ mothand     : int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ fathhand    : int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ hairappt    : int  13 13 13 13 10 10 10 10 20 20 ...\n#>  $ songs       : int  20 20 20 20 10 10 10 10 25 25 ...\n#>  $ gpa_grade   : int  4 4 4 4 1 1 1 1 2 2 ...\n#>  $ gpa         : num  4 4 4 4 3 3 3 3 3.6 3.6 ...\n#>  $ exercise    : int  3 3 3 3 6 6 6 6 4 4 ...\n#>  $ exercise_cat: int  3 3 3 3 4 4 4 4 3 3 ...\n#>  $ polview     : int  4 4 4 4 2 2 2 2 3 3 ...\n#>  $ tv          : int  6 6 6 6 15 15 15 15 8 8 ...\n#>  $ coffee      : int  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ sleep       : num  9 9 9 9 6 6 6 6 6 6 ...\n#>  $ drinks      : num  0.5 0.5 0.5 0.5 0 0 0 0 5 5 ...\n#>  $ pepsicok    : int  1 1 1 1 0 0 0 0 1 1 ...\n#>  $ haircol     : int  3 3 3 3 1 1 1 1 3 3 ...\n#>  $ eyecolor    : int  3 3 3 3 3 3 3 3 2 2 ...\n#>  $ distance    : num  1 1 1 1 60 60 60 60 80 80 ...\n#>  $ distance_cat: int  1 1 1 1 2 2 2 2 3 3 ...\n#>  $ books       : int  0 0 0 0 0 0 0 0 1 1 ...\n#>  $ studyhrs    : int  3 3 3 3 3 3 3 3 4 4 ...\n#>  $ studyhrs_cat: int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ mostint     : int  16 16 16 16 10 10 10 10 6 6 ...\n#>  $ leastint    : int  8 8 8 8 13 13 13 13 3 3 ..."},{"path":"loading-data-in-r.html","id":"importing-spss-and-stata-file","chapter":"3 Loading Data in R","heading":"3.3 Importing SPSS and STATA file","text":"R also package called haven helps us read SPSS STATA data files easily R. installing haven package, use read_sav command import SPSS file.","code":"\n  #Install package\n  install.packages('haven')\n  \n  #Load the package and read SPSS data file\n  \n  library(haven)\n  savdata1 <- read_sav('ancova.sav')\n  \n  #To verify the file has been imported successfully.\n  savdata1\n  \n  #Load the package and read STATA data file\n  \n  library(haven)\n  dtadata1 <- read_dta('ancovastata.dta')\n  \n  #To verify the file has been imported successfully.\n  dtadata1\n  "},{"path":"loading-data-in-r.html","id":"importing-excel-file","chapter":"3 Loading Data in R","heading":"3.4 Importing Excel File","text":"readxl package used read excel file R environment.R comprehensive packages import multiple statistical systems. packages include foreign, readdta1 etc. Find Data Import Export R .","code":"\n #Install package\n  install.packages('readxl')\n  \n  #Load the package and read data\n  \n  library(readxl)\n  xlsdata1 <- read_excel('C:\\\\Users\\\\para\\\\Downloads\\\\ancova.xls')\n  \n  #To verify the file has been imported successfully.\n  xlsdata1\n  "},{"path":"data-representation.html","id":"data-representation","chapter":"4 Data Representation","heading":"4 Data Representation","text":"Data representation refers process presenting data visual graphical format makes easier understand interpret. crucial effectively represent data communicate research findings, identify trends, explore relationships variables.Examples data representation techniques educational psychology might include creating line graphs show changes student performance time, using pie charts compare student achievement across different demographic groups, using scatter plots explore relationship two variables study.ggplot2 powerful data visualization package R, created Hadley Wickham. based Grammar Graphics, framework allows build complex customizable plots layering components. ggplot2 enables creation wide variety visually appealing informative graphics relatively concise consistent syntax.","code":""},{"path":"data-representation.html","id":"loading-your-data","chapter":"4 Data Representation","heading":"4.1 Loading your data","text":"can import load data discussed Chapter 3. examples, loading gapminder data set available package R.","code":"\n# Load required packages\nlibrary(ggplot2)\nlibrary(gapminder)\n\n# Preview the dataset\nhead(gapminder)"},{"path":"data-representation.html","id":"frequency-tables","chapter":"4 Data Representation","heading":"4.2 Frequency Tables","text":"frequency table displays number occurrences (frequencies) category value data set. particularly useful summarizing categorical data discrete numerical data.","code":"\n# Load required packages\nlibrary(ggplot2)#> Warning: package 'ggplot2' was built under R version 4.2.3\nlibrary(gapminder)#> Warning: package 'gapminder' was built under R version\n#> 4.2.3\n# Create a frequency table for continent\ncontinent_freq <- table(gapminder$continent)\ncontinent_freq#> \n#>   Africa Americas     Asia   Europe  Oceania \n#>      624      300      396      360       24"},{"path":"data-representation.html","id":"histograms","chapter":"4 Data Representation","heading":"4.3 Histograms:","text":"Histograms used visualize distribution continuous discrete numerical data. display data using intervals (bins) along x-axis frequency observations within bin y-axis.aes() function sets aesthetic mappings plot. case, x-axis mapped lifeExp variable dataset, represents life expectancy.geom_histogram(binwidth = 5)adds histogram layer plot. binwidth parameter set 5, means data divided bins width 5. height bar histogram represents frequency (count) data points within bin.xlab(\"Life Expectancy\") adds label x-axis, naming “Life Expectancy”.ylab(\"Frequency\")adds label y-axis, naming “Frequency”.","code":"\n# Histogram for life expectancy using ggplot2\nggplot(gapminder, aes(x = lifeExp)) + geom_histogram(binwidth = 5) + xlab(\"Life Expectancy\") + ylab(\"Frequency\")"},{"path":"data-representation.html","id":"bar-graphs","chapter":"4 Data Representation","heading":"4.4 Bar Graphs:","text":"Bar graphs used displaying categorical data. category represented bar, height (length) bar indicates frequency count category.","code":"\n# Bar graph of continents using ggplot2\nggplot(gapminder, aes(x = continent)) + geom_bar()"},{"path":"data-representation.html","id":"pie-charts","chapter":"4 Data Representation","heading":"4.5 Pie Charts:","text":"Pie charts represent categorical data slices circle. size slice proportional frequency category. Pie charts useful visualizing relative proportions categories. Drawing piechart ggplot2 package requires transforming bar plot polar coordinates, however, much easier plotrix package. can install package using install.packages(plotrix).","code":"\n# Load necessary package\nlibrary(plotrix)\n\n# Pie chart for continents\npie3D(table(gapminder$continent), labels = names(table(gapminder$continent)), main = \"Proportion of Continents\")"},{"path":"data-representation.html","id":"box-plots","chapter":"4 Data Representation","heading":"4.6 Box Plots:","text":"Box plots used visualizing distribution continuous discrete numerical data. show median, quartiles, outliers data, providing compact informative representation data distribution.","code":"\n# Box plot of life expectancy by continent using ggplot2\nggplot(gapminder, aes(x = continent, y = lifeExp)) + geom_boxplot()"},{"path":"data-representation.html","id":"scatter-plots","chapter":"4 Data Representation","heading":"4.7 Scatter Plots","text":"Scatter plots used display relationship two continuous variables. can particularly helpful identifying trends, correlations, potential outliers data.","code":"\n# Scatter plot of life expectancy vs. GDP per capita using ggplot2\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10()"},{"path":"data-representation.html","id":"line-graphs","chapter":"4 Data Representation","heading":"4.8 Line Graphs","text":"Line graphs used display relationship continuous variable discrete ordinal variable, often representing change time. can particularly useful identifying trends patterns time-series data.","code":"\n# Line graph of average life expectancy over time using ggplot2\ngapminder_agg <- aggregate(lifeExp ~ year, data = gapminder, mean)\nggplot(gapminder_agg, aes(x = year, y = lifeExp)) + geom_line()"},{"path":"describing-data-in-r.html","id":"describing-data-in-r","chapter":"5 Describing Data in R","heading":"5 Describing Data in R","text":"Describing data process summarizing interpreting information collected research observation.describe data, typically use statistical measures measures central tendency (e.g., mean, median, mode) measures variability (e.g., range, variance, standard deviation). measures help us understand distribution data, including data spread whether skewed symmetrical.describing data, ’s important consider research question hypothesis investigated. focusing relevant aspects data using appropriate statistical visual tools, can better understand underlying trends relationships data draw meaningful conclusions can inform future research educational practice.","code":""},{"path":"describing-data-in-r.html","id":"central-tendency","chapter":"5 Describing Data in R","heading":"5.1 Central Tendency:","text":"Central tendency measures provide single value represents center “typical” value dataset. primary measures central tendency mean, median, mode.","code":""},{"path":"describing-data-in-r.html","id":"sample-mean","chapter":"5 Describing Data in R","heading":"5.1.1 Sample Mean","text":"sample mean average value variable sample. denoted symbol “x̄”.sample mean, often referred average, sum data points divided total number data points. mean sensitive extreme values (outliers) may always represent true center data.\\(\\bar{x} = \\frac{1}{n}\\sum_{=1}^{n} x_i\\)\\(\\bar{x}\\) mean, \\(n\\) total number data points, \\(x_i\\) individual data points.R, can calculate mean using mean() function. example:","code":"\n# Example: Mean test scores of all students in a school\nscores <- c(80, 85, 90, 75, 95, 85, 70, 75, 90, 80)\nmean(scores)\n#> [1] 82.5"},{"path":"describing-data-in-r.html","id":"median","chapter":"5 Describing Data in R","heading":"5.1.2 Median","text":"median middle value dataset sorted ascending descending order. dataset odd number data points, median middle value; even number data points, median average two middle values. median less sensitive extreme values compared mean.t represented :\\(median = \\begin{cases} x_{(n+1)/2} &\\text{}n\\text{ odd}\\ \\frac{x_{n/2} + x_{(n/2)+1}}{2} &\\text{}n\\text{ even} \\end{cases}\\)\\(n\\) total number data points, \\(x_{(n+1)/2}\\) \\(x_{n/2}\\) middle values odd even \\(n\\) respectively.","code":"\n# Example: Median income\nincome <- c(25000, 30000, 35000, 40000, 45000, 50000) # income data\nmedian(income) # calculate the median\n#> [1] 37500"},{"path":"describing-data-in-r.html","id":"mode","chapter":"5 Describing Data in R","heading":"5.1.3 Mode","text":"mode value occurs frequently dataset. dataset can one mode (multimodal) mode (value occurs ). mode can used numerical categorical data.","code":""},{"path":"describing-data-in-r.html","id":"measures-of-dispersion","chapter":"5 Describing Data in R","heading":"5.2 Measures of Dispersion","text":"Measures dispersion statistical values describe degree spread variability dataset. several measures dispersion, including range, interquartile range, variance, standard deviation. section, discuss measure dispersion formula detail.","code":""},{"path":"describing-data-in-r.html","id":"range","chapter":"5 Describing Data in R","heading":"5.2.1 Range:","text":"range simplest measure dispersion gives difference maximum minimum values dataset. provides idea spread data . However, highly sensitive outliers provide information distribution’s shape. formula range follows:range() function returns difference maximum minimum values dataset, providing measure spread data around .","code":"\n# Example dataset\nincome <- c(25000, 30000, 35000, 40000, 45000, 50000)\n# Calculate range\nrange(income)\n#> [1] 25000 50000"},{"path":"describing-data-in-r.html","id":"interquartile-range-iqr","chapter":"5 Describing Data in R","heading":"5.2.2 Interquartile Range (IQR):","text":"interquartile range (IQR) measure dispersion indicates spread middle 50% data. less sensitive outliers range. IQR defined difference first quartile (Q1, 25th percentile) third quartile (Q3, 75th percentile) dataset. formula IQR follows:Q1 first quartile Q3 third quartile dataset.IQR() function calculates difference 75th percentile (Q3) 25th percentile (Q1) dataset, providing measure spread middle 50% data.","code":"\nIQR(income)\n#> [1] 12500"},{"path":"describing-data-in-r.html","id":"variance","chapter":"5 Describing Data in R","heading":"5.2.3 Variance:","text":"variance measure dispersion quantifies average deviation data points mean. measures far set numbers spread average value. variance expressed squared units influenced outliers. two types variance: population variance sample variance.Population variance used entire population available, sample variance used sample population available. formula population variance follows:X data point, μ mean dataset, N total number data points population.formula sample variance slightly different follows:X data point, \\(\\bar{x}\\) mean sample, n sample size.var() function used calcultate variance.","code":"\n# Calculate variance\nvar(income)\n#> [1] 87500000"},{"path":"describing-data-in-r.html","id":"standard-deviation","chapter":"5 Describing Data in R","heading":"5.2.4 Standard Deviation:","text":"standard deviation square root variance. widely used measure dispersion measures average deviation data points mean. expressed units data sensitive outliers, just like variance. two types standard deviation: population standard deviation sample standard deviation.formula population standard deviation follows:\\(\\sigma^2\\) population variance.formula sample standard deviation slightly different follows:s^2 sample variance.sd() function used calculate standard deviation.","code":"\n# Calculate standard deviation\nsd(income)\n#> [1] 9354.143"},{"path":"describing-data-in-r.html","id":"the-psych-package---kitchen-knife-of-social-scientists","chapter":"5 Describing Data in R","heading":"5.3 The psych package - Kitchen knife of social scientists","text":"psych package R popular package psychometrics psychological research. contains various functions data manipulation, visualization, statistical analysis. commonly used functions psych package include describe(), alpha(), fa().","code":""},{"path":"describing-data-in-r.html","id":"the-describe-function","chapter":"5 Describing Data in R","heading":"5.3.1 The describe() Function","text":"describe() function psych package provides summary dataset’s variables. includes number observations, mean, standard deviation, minimum, maximum, useful statistics. ’s example:Descriptive income datasetWe use College dataset ISLR package (James et al., 2021) . dataset contains information number colleges US, including variables acceptance rate, graduation rate, median SAT score.","code":"\n# Load required packages and dataset\nlibrary(psych)\n#> Warning: package 'psych' was built under R version 4.2.3\nlibrary(ISLR)\n#> Warning: package 'ISLR' was built under R version 4.2.3\ndata(\"College\") \n\n#view summary of the income data\ndescribe(income)\n#>    vars n  mean      sd median trimmed     mad   min   max\n#> X1    1 6 37500 9354.14  37500   37500 11119.5 25000 50000\n#>    range skew kurtosis      se\n#> X1 25000    0     -1.8 3818.81\n# View summary statistics of the college dataset\ndescribe(College)\n#>             vars   n     mean      sd median  trimmed\n#> Private*       1 777     1.73    0.45    2.0     1.78\n#> Apps           2 777  3001.64 3870.20 1558.0  2193.01\n#> Accept         3 777  2018.80 2451.11 1110.0  1510.29\n#> Enroll         4 777   779.97  929.18  434.0   575.95\n#> Top10perc      5 777    27.56   17.64   23.0    25.13\n#> Top25perc      6 777    55.80   19.80   54.0    55.12\n#> F.Undergrad    7 777  3699.91 4850.42 1707.0  2574.88\n#> P.Undergrad    8 777   855.30 1522.43  353.0   536.36\n#> Outstate       9 777 10440.67 4023.02 9990.0 10181.66\n#> Room.Board    10 777  4357.53 1096.70 4200.0  4301.70\n#> Books         11 777   549.38  165.11  500.0   535.22\n#> Personal      12 777  1340.64  677.07 1200.0  1268.35\n#> PhD           13 777    72.66   16.33   75.0    73.92\n#> Terminal      14 777    79.70   14.72   82.0    81.10\n#> S.F.Ratio     15 777    14.09    3.96   13.6    13.94\n#> perc.alumni   16 777    22.74   12.39   21.0    21.86\n#> Expend        17 777  9660.17 5221.77 8377.0  8823.70\n#> Grad.Rate     18 777    65.46   17.18   65.0    65.60\n#>                 mad    min     max   range  skew kurtosis\n#> Private*       0.00    1.0     2.0     1.0 -1.02    -0.96\n#> Apps        1463.33   81.0 48094.0 48013.0  3.71    26.52\n#> Accept      1008.17   72.0 26330.0 26258.0  3.40    18.75\n#> Enroll       354.34   35.0  6392.0  6357.0  2.68     8.74\n#> Top10perc     13.34    1.0    96.0    95.0  1.41     2.17\n#> Top25perc     20.76    9.0   100.0    91.0  0.26    -0.57\n#> F.Undergrad 1441.09  139.0 31643.0 31504.0  2.60     7.61\n#> P.Undergrad  449.23    1.0 21836.0 21835.0  5.67    54.52\n#> Outstate    4121.63 2340.0 21700.0 19360.0  0.51    -0.43\n#> Room.Board  1005.20 1780.0  8124.0  6344.0  0.48    -0.20\n#> Books        148.26   96.0  2340.0  2244.0  3.47    28.06\n#> Personal     593.04  250.0  6800.0  6550.0  1.74     7.04\n#> PhD           17.79    8.0   103.0    95.0 -0.77     0.54\n#> Terminal      14.83   24.0   100.0    76.0 -0.81     0.22\n#> S.F.Ratio      3.41    2.5    39.8    37.3  0.66     2.52\n#> perc.alumni   13.34    0.0    64.0    64.0  0.60    -0.11\n#> Expend      2730.95 3186.0 56233.0 53047.0  3.45    18.59\n#> Grad.Rate     17.79   10.0   118.0   108.0 -0.11    -0.22\n#>                 se\n#> Private*      0.02\n#> Apps        138.84\n#> Accept       87.93\n#> Enroll       33.33\n#> Top10perc     0.63\n#> Top25perc     0.71\n#> F.Undergrad 174.01\n#> P.Undergrad  54.62\n#> Outstate    144.32\n#> Room.Board   39.34\n#> Books         5.92\n#> Personal     24.29\n#> PhD           0.59\n#> Terminal      0.53\n#> S.F.Ratio     0.14\n#> perc.alumni   0.44\n#> Expend      187.33\n#> Grad.Rate     0.62"},{"path":"normal-distribution.html","id":"normal-distribution","chapter":"6 Normal Distribution","heading":"6 Normal Distribution","text":"normal distribution fundamental concept statistics probability theory. bell-shaped, continuous probability distribution symmetrical fully determined two parameters: mean (μ) standard deviation (σ). normal distribution characterized following properties:continuous takes possible real values.symmetric around mean, highest point curve.bell-shaped curve, data clustered around mean gradually decreasing frequency move away mean.fully specified mean standard deviation, 68-95-99.7 rule describing much data falls within one, two, three standard deviations mean.widely used model many natural phenomena measurements social natural sciences.One key properties normal distribution 68-95-99.7 rule, also known empirical rule three-sigma rule. According rule, approximately 68% data falls within one standard deviation mean, 95% data falls within two standard deviations mean, 99.7% data falls within three standard deviations mean. property makes normal distribution useful making probabilistic statements data.Normal distribution continuous, symmetric, bell-shaped\ndistribution defined two parameters: mean (μ) \nstandard deviation (σ).mean (μ) determines center distribution, standard\ndeviation (σ) controls spread dispersion data.\nApproximately 68% data falls within one standard deviation \nmean, 95% falls within two standard deviations, 99.7% falls\nwithin three standard deviations. known Empirical Rule \n68-95-99.7 rule.probability density function (PDF) Normal distribution \ngiven ::f(x) probability density point x μ mean \ndistribution σ standard deviation distribution e \nbase natural logarithm (approximately 2.718) π \nmathematical constant Pi (approximately 3.141) .","code":""},{"path":"normal-distribution.html","id":"unit-normal-distribution","chapter":"6 Normal Distribution","heading":"6.1 Unit Normal Distribution","text":"unit normal distribution normal distribution mean 0 standard deviation 1. also known standard normal distribution Z-distribution. concept unit normal distribution useful normal distribution can transformed unit normal distribution process called standardization. Standardization involves subtracting mean data point dividing standard deviation. transforms data Z-score, represents number standard deviations away mean.plot shows thebell-shaped curve distribution, illustrating data \nsymmetric around mean decreases move away center.normal distribution relates concepts skewness kurtosis. Skewness refers degree asymmetry distribution, positive skewness indicating distribution skewed right negative skewness indicating distribution skewed left. normal distribution skewness 0, indicating perfect symmetry. Kurtosis refers degree peakedness distribution, high kurtosis indicating sharp peak low kurtosis indicating flat peak. normal distribution kurtosis 3, known mesokurtic. Distributions higher kurtosis called leptokurtic lower kurtosis called platykurtic.","code":"#> Warning: package 'ggplot2' was built under R version 4.2.3"},{"path":"skewness-and-kurtosis.html","id":"skewness-and-kurtosis","chapter":"7 Skewness and Kurtosis","heading":"7 Skewness and Kurtosis","text":"Skewness Kurtosis two different measures shapes \ndistribution dataset qualitative methods.","code":""},{"path":"skewness-and-kurtosis.html","id":"skewness","chapter":"7 Skewness and Kurtosis","heading":"7.1 Skewness","text":"Skewness measure asymmetry distribution. describes\ndegree distribution deviates symmetric shape. \nskewness value 0 indicates perfectly symmetric distribution.\nPositive skewness indicates distribution longer tail \nright side, negative skewness indicates longer tail left\nside.","code":""},{"path":"skewness-and-kurtosis.html","id":"kurtosis","chapter":"7 Skewness and Kurtosis","heading":"7.2 Kurtosis","text":"Kurtosis measure “tailedness” “peakedness” \ndistribution. describes distribution’s tails peak compare\nnormal distribution. kurtosis value 0 indicates distribution\nsimilar shape normal distribution. Positive kurtosis\nindicates distribution heavier tails peaked shape \nnormal distribution, negative kurtosis indicates lighter tails\nless peaked shape.","code":""},{"path":"skewness-and-kurtosis.html","id":"generating-skewness-and-kurtosis-using-r","chapter":"7 Skewness and Kurtosis","heading":"7.3 Generating Skewness and Kurtosis using R","text":"can use psych package generate skewness Kurtosis. describe() function also provides descriptive statistics, mean, median, standard deviation, range. want see skewness kurtosis, can use skew() kurtosis() functions separately.","code":""},{"path":"standard-scores.html","id":"standard-scores","chapter":"8 Standard Scores","heading":"8 Standard Scores","text":"Standard scores type transformed scores express individual data points dataset relative mean standard deviation dataset. Standard scores allow comparing scores across different distributions scales placing common scale. provide standardized measure position data point within distribution, taking account average value (mean) spread (standard deviation) data.","code":""},{"path":"standard-scores.html","id":"z--score","chapter":"8 Standard Scores","heading":"8.1 Z- Score","text":"z-score type standard score calculated subtracting mean (μ) individual data point (X) dividing result standard deviation (σ):z = (X - μ) / σA z-score represents many standard deviations data point mean. positive z-score indicates data point mean, negative z-score indicates mean. z-score 0 corresponds mean distribution.educational settings, z-scores can used various ways, :Comparing student performance: Z-scores enable comparison student scores across different tests grading scales standardizing scores. allows educators make informed decisions student performance identify students might need additional support resources.Identifying outliers: Z-scores can help identify students perform exceptionally well poorly compared group mean. Outliers can provide insights effectiveness teaching methods, identify areas improvement, recognize exceptional talent.Normalizing grades: cases distribution grades skewed, converting raw scores z-scores can provide equitable assessment student performance. Z-scores can converted percentiles, represent percentage students scored lower particular student, providing standardized ranking within group.calculate z-scores R, can use following code:understanding utilizing z-scores education, educators researchers can make informed decisions student performance, compare results across different assessments, identify patterns trends student achievement.","code":"\n# Example data\ndata <- c(60, 65, 70, 75, 80, 85, 90)\n\n# Calculate the mean and standard deviation\nmean_data <- mean(data)\nsd_data <- sd(data)\n\n# Calculate z-scores\nz_scores <- (data - mean_data) / sd_data\n\nz_scores\n#> [1] -1.3887301 -0.9258201 -0.4629100  0.0000000  0.4629100\n#> [6]  0.9258201  1.3887301"},{"path":"standard-scores.html","id":"t--scores","chapter":"8 Standard Scores","heading":"8.2 T- Scores","text":"T-score type standard score used transform standardize individual data points dataset. T-scores similar z-scores, use different scaling factor place scores specific scale. T-scores especially helpful comparing scores across different distributions scales.T-score calculated subtracting mean (μ) individual data point (X), dividing result standard deviation (σ), multiplying result scaling factor (usually 10) adding constant (usually 50):T = ((X - μ) / σ) * 10 + 50The scaling factor 10 constant 50 ensure T-scores mean 50 standard deviation 10. T-score transformation preserves shape original distribution relative positions data points.T-scores can used various ways:Comparing scores across different tests scales: T-scores enable comparison scores different tests grading scales standardizing scores common scale. allows meaningful comparisons helps decision-making considering different assessments.Norm-referenced interpretation: T-scores often used standardized testing provide norm-referenced interpretation test scores. enables comparison individual’s performance performance reference group (e.g., age grade peers).Clinical psychological assessments: T-scores commonly used clinical psychological assessments interpret scores various tests questionnaires, allowing practitioners compare individual’s performance symptoms normative sample.calculate T-scores R, can use following code:understanding utilizing T-scores, can make informed decisions individual performance, compare results across different assessments, identify patterns trends standardized manner.","code":"\n# Example data\ndata <- c(60, 65, 70, 75, 80, 85, 90)\n\n# Calculate the mean and standard deviation\nmean_data <- mean(data)\nsd_data <- sd(data)\n\n# Calculate T-scores\nt_scores <- ((data - mean_data) / sd_data) * 10 + 50"},{"path":"probability-and-inference.html","id":"probability-and-inference","chapter":"9 Probability and Inference","heading":"9 Probability and Inference","text":"","code":""},{"path":"probability-and-inference.html","id":"probability","chapter":"9 Probability and Inference","heading":"9.1 Probability:","text":"Probability numerical measure likelihood particular event occur. ranges 0 1, 0 meaning event impossible 1 meaning event certain. Probabilities can represented graphically using bar plots pie charts.","code":""},{"path":"probability-and-inference.html","id":"sample-space","chapter":"9 Probability and Inference","heading":"9.2 Sample Space:","text":"sample space set possible outcomes given experiment event. example, coin toss experiment, sample space {Heads, Tails}. sample space can represented using Venn diagrams tree diagrams.","code":""},{"path":"probability-and-inference.html","id":"conditional-probability","chapter":"9 Probability and Inference","heading":"9.3 Conditional Probability:","text":"Conditional probability refers probability event occurring given another event already occurred. can represented graphically using Venn diagrams, show intersections events.","code":""},{"path":"probability-and-inference.html","id":"independence","chapter":"9 Probability and Inference","heading":"9.4 Independence:","text":"Two events independent occurrence one event affect probability event. Graphically, independence can visualized using Venn diagrams probability tables, probability intersection two events equal product individual probabilities.","code":""},{"path":"probability-and-inference.html","id":"bayes-theorem","chapter":"9 Probability and Inference","heading":"9.5 Bayes’ Theorem:","text":"Bayes’ theorem powerful tool updating probability event based new evidence. can visualized using tree diagrams probability tables, show updated probabilities taking account new evidence.","code":""},{"path":"probability-and-inference.html","id":"discrete-and-continuous-probability-distributions","chapter":"9 Probability and Inference","heading":"9.6 Discrete and Continuous Probability Distributions:","text":"Discrete probability distributions describe probabilities outcomes discrete random variables (e.g., number heads coin tosses), continuous probability distributions describe probabilities outcomes continuous random variables (e.g., height individuals). Discrete distributions can visualized using bar plots, continuous distributions can visualized using probability density functions cumulative distribution functions.#Sampling distributionA sampling distribution probability distribution sample statistic (e.g., sample mean, sample proportion) obtained population. helps understand variability sample statistic likelihood obtaining different sample statistics population.","code":""},{"path":"probability-and-inference.html","id":"central-limit-theorem","chapter":"9 Probability and Inference","heading":"9.7 Central Limit Theorem","text":"CLT states , large enough sample size (usually n ≥ 30), distribution sample means approaches normal distribution, regardless shape population distribution. mean sampling distribution equal population mean (μ), standard deviation (standard error) equal population standard deviation (σ) divided square root sample size (n).","code":"\n# Load required libraries\nlibrary(ggplot2)\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Define population parameters\npopulation_mean <- 10\npopulation_sd <- 5\n\n# Define sample size and number of samples\nsample_size <- 50\nnum_samples <- 1000\n\n# Generate random samples and calculate sample means\nsample_means <- replicate(num_samples, mean(rnorm(sample_size, mean = population_mean, sd = population_sd)))\n\n# Plot the distribution of sample means\nggplot(data.frame(sample_means), aes(x = sample_means)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"blue\") +\n  ggtitle(\"Sampling Distribution of the Mean\") +\n  xlab(\"Sample Means\") +\n  ylab(\"Density\") +\n  theme_minimal()\n#> Warning: The dot-dot notation (`..density..`) was deprecated in\n#> ggplot2 3.4.0.\n#> ℹ Please use `after_stat(density)` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated."},{"path":"probability-and-inference.html","id":"confidence-intervals","chapter":"9 Probability and Inference","heading":"9.8 Confidence Intervals","text":"Confidence intervals range values within true population parameter likely fall, specified level confidence (e.g., 95%). Confidence intervals provide estimate precision uncertainty sample statistic.","code":"\n# Define the sample data\nsample_data <- c(12, 15, 18, 20, 22, 24, 25, 28, 30, 32)\n\n# Calculate the sample mean and standard deviation\nsample_mean <- mean(sample_data)\nsample_sd <- sd(sample_data)\n\n# Calculate the standard error\nstandard_error <- sample_sd / sqrt(length(sample_data))\n\n# Calculate the 95% confidence interval\nalpha <- 0.05\ncritical_value <- qnorm(1 - alpha / 2)\nmargin_of_error <- critical_value * standard_error\nconfidence_interval <- c(sample_mean - margin_of_error, sample_mean + margin_of_error)"},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"10 Hypothesis Testing","heading":"10 Hypothesis Testing","text":"Hypothesis testing method used make decisions population parameters based sample data.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis","chapter":"10 Hypothesis Testing","heading":"10.1 Hypothesis","text":"hypothesis educated guess statement relationship variables characteristics population. hypothesis testing, two main hypotheses:","code":""},{"path":"hypothesis-testing.html","id":"null-hypothesis-h0","chapter":"10 Hypothesis Testing","heading":"10.1.1 Null hypothesis (H0):","text":"hypothesis states effect relationship variables. typically hypothesis researcher wants disprove.","code":""},{"path":"hypothesis-testing.html","id":"alternative-hypothesis-h1","chapter":"10 Hypothesis Testing","heading":"10.1.2 Alternative hypothesis (H1):","text":"hypothesis states effect relationship variables. hypothesis researcher wants prove provide evidence .","code":""},{"path":"hypothesis-testing.html","id":"decision-type-error","chapter":"10 Hypothesis Testing","heading":"10.2 Decision Type Error","text":"performing hypothesis testing, two types decision errors:Type Error (α): error occurs null hypothesis rejected actually true. words, ’s false positive. probability committing Type error denoted significance level (α), typically set 0.05 0.01.\nType II Error (β): error occurs null hypothesis rejected actually false. words, ’s false negative. probability committing Type II error denoted β. power test (1 - β) measures ability test detect effect truly exists.\ngraphical representation types decision errors:Hypothesis Testing ErrorsThis table represents different outcomes making decisions based hypothesis testing. columns represent reality (.e., whether null hypothesis true false), rows represent decision made based hypothesis test (.e., whether reject reject null hypothesis). cells show types decision errors (Type Type II errors) correct decisions.","code":"|                  | Null Hypothesis (H0) is True | Null Hypothesis (H0) is False |\n|------------------|------------------------------|-------------------------------|\n| Reject H0        | Type I Error (α)             | Correct Decision (1 - β)      |\n| Fail to Reject H0| Correct Decision (1 - α)     | Type II Error (β)             |"},{"path":"hypothesis-testing.html","id":"level-of-signficance","chapter":"10 Hypothesis Testing","heading":"10.3 Level of Signficance","text":"level significance critical component hypothesis testing sets threshold determining whether observed effect statistically significant .level significance denoted Greek letter α (alpha) represents probability making Type error. Type error occurs reject null hypothesis (H0) actually true. choosing level significance, researchers define risk willing take rejecting true null hypothesis. Common levels significance 0.05 (5%) 0.01 (1%).better understand role level significance hypothesis testing, let’s consider following steps:Formulate null hypothesis (H0) alternative hypothesis (H1): null hypothesis typically states effect relationship variables, alternative hypothesis states effect relationship.Choose level significance (α): Determine threshold probability making Type error. example, α set 0.05, 5% chance rejecting true null hypothesis.Perform statistical test calculate test statistic: test statistic calculated using sample data, helps determine far observed sample mean hypothesized population mean. case single mean, one-sample t-test commonly used, test statistic t-value.Determine critical value p-value: Compare calculated test statistic critical value p-value (probability value) make decision null hypothesis. critical value threshold value depends chosen level significance distribution test statistic. p-value represents probability obtaining test statistic extreme extreme observed test statistic assumption null hypothesis true.Make decision: test statistic extreme critical value, p-value less level significance (α), reject null hypothesis. Otherwise, fail reject null hypothesis.","code":""},{"path":"hypothesis-testing.html","id":"t-statistic","chapter":"10 Hypothesis Testing","heading":"10.4 T-statistic","text":"t-statistic standardized measure used hypothesis testing compare observed sample mean hypothesized population mean. takes account sample mean, hypothesized population mean, standard error mean. Mathematically, t-statistic can calculated using following formula:t = (X̄ - μ) / (s / √n):t t-statistic\nX̄ sample mean\nμ hypothesized population mean\ns sample standard deviation\nn sample size","code":""},{"path":"hypothesis-testing.html","id":"t-distribution","chapter":"10 Hypothesis Testing","heading":"10.4.1 T-distribution","text":"t-distribution, also known Student’s t-distribution, probability distribution used population standard deviation unknown sample size small. similar normal distribution thicker tails, accounts increased variability due using sample standard deviation estimate population standard deviation. shape t-distribution depends degrees freedom (df), related sample size (df = n - 1). sample size increases, t-distribution approaches normal distribution.calculate t-statistic R, can use following code:perform one-sample t-test R, calculates t-statistic p-value automatically, can use t.test() function:","code":"\n# Sample data\ndata <- c(12, 14, 16, 18, 20)\n\n# Hypothesized population mean\nhypothesized_mean <- 15\n\n# Calculate the sample mean, standard deviation, and size\nsample_mean <- mean(data)\nsample_sd <- sd(data)\nsample_size <- length(data)\n\n# Calculate the t-statistic\nt_statistic <- (sample_mean - hypothesized_mean) / (sample_sd / sqrt(sample_size))\n\n# Print the t-statistic\nprint(t_statistic)\n#> [1] 0.7071068\n# Perform a one-sample t-test\nt_test_result <- t.test(data, mu = hypothesized_mean)\n\n# Print the t-test result\nprint(t_test_result)\n#> \n#>  One Sample t-test\n#> \n#> data:  data\n#> t = 0.70711, df = 4, p-value = 0.5185\n#> alternative hypothesis: true mean is not equal to 15\n#> 95 percent confidence interval:\n#>  12.07351 19.92649\n#> sample estimates:\n#> mean of x \n#>        16"},{"path":"hypothesis-testing.html","id":"intepreting-normality-evidence","chapter":"10 Hypothesis Testing","heading":"10.4.2 Intepreting Normality Evidence","text":"using t-test, assumption normality important. data follow normal distribution ensure validity test results. assess normality data, can use visual methods (histograms, Q-Q plots) statistical tests (e.g., Shapiro-Wilk test).important t-test assumes data follow normal distribution, verifying assumption helps ensure validity test results.generate normality evidence performing t-test, can use following methods:Visual methods: Histograms Q-Q plots can provide visual assessment normality data.Statistical tests: Shapiro-Wilk test Kolmogorov-Smirnov test commonly used test normality. tests generate p-values, can compared chosen significance level (e.g., 0.05) determine data deviate significantly normality.R, can create histogram Q-Q plot using following code:Create histogram Q-Q plot:Perform Shapiro-Wilk test:interpret normality evidence, follow guidelines:Visual methods: Inspect histogram Q-Q plot. histogram roughly bell-shaped points Q-Q plot fall approximately reference line, data can considered approximately normally distributed.Statistical tests: Check p-values normality tests. p-value greater chosen significance level (e.g., 0.05), null hypothesis (.e., data follow normal distribution) rejected. suggests data deviate significantly normality.Keep mind single method foolproof, ’s often good idea use combination visual statistical methods assess normality. data appear non-normal, might consider using non-parametric alternatives t-test transforming data achieve normality.","code":"\n# Load required libraries\nlibrary(ggplot2)\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n\n# Sample data\ndata <- c(12, 14, 16, 18, 20)\n\n# Create a histogram\nggplot(data.frame(data), aes(data)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"white\") +\n  theme_minimal()\n\n# Create a Q-Q plot\nqqnorm(data)\nqqline(data, col = \"red\")\n# Perform the Shapiro-Wilk test\nshapiro_test_result <- shapiro.test(data)\n\n# Print the test result\nprint(shapiro_test_result)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  data\n#> W = 0.98676, p-value = 0.9672"},{"path":"hypothesis-testing.html","id":"statistical-power","chapter":"10 Hypothesis Testing","heading":"10.5 Statistical Power","text":"Statistical power probability correctly rejecting null hypothesis false, means committing Type II error. Power influenced factors sample size, effect size, chosen significance level (α). Power analysis helps researchers determine appropriate sample size needed achieve desired level power, typically 0.8 higher.perform power analysis R, can use pwr package, provides set functions power calculations various statistical tests, including t-test.’s step--step procedure generating testing power using R:Install load pwr package:Define parameters power analysis. need specify effect size (Cohen’s d), sample size, significance level (α):Use pwr.t.test() function calculate power one-sample t-test:output show calculated power, sample size, effect size, significance level. power desired level (e.g., 0.8), can adjust sample size effect size recalculate power determine necessary changes achieving desired power level.’s essential consider practical implications effect size sample size planning study. large effect size may easier detect might occur frequently real-world situations. Conversely, small effect size might difficult detect may require larger sample size achieve adequate power.","code":"\n\n# Load the pwr package\nlibrary(pwr)\n#> Warning: package 'pwr' was built under R version 4.2.3\n# Define parameters for power analysis\neffect_size <- 0.8  # Cohen's d\nsample_size <- 20\nsignificance_level <- 0.05\n# Calculate the power for a one-sample t-test\npower_result <- pwr.t.test(n = 500,\n                           d = effect_size,\n                           sig.level = significance_level,\n                           type = \"one.sample\",\n                           alternative = \"two.sided\")\n\n# Print the power result\nprint(power_result)\n#> \n#>      One-sample t test power calculation \n#> \n#>               n = 500\n#>               d = 0.8\n#>       sig.level = 0.05\n#>           power = 1\n#>     alternative = two.sided"},{"path":"independent-samples-t---test.html","id":"independent-samples-t---test","chapter":"11 Independent Samples T - Test","heading":"11 Independent Samples T - Test","text":"table provides comprehensive comparison dependent independent samples, including definitions, examples, hypothesis testing, assumptions, statistical tests, effect size measures, R functions. understanding differences, can choose appropriate statistical test data interpret results correctly.","code":""},{"path":"independent-samples-t---test.html","id":"independent-samples-t-test","chapter":"11 Independent Samples T - Test","heading":"11.1 Independent Samples t-test","text":"independent samples t-test used compare means two independent groups determine significant difference .independent samples t-test based following null (H₀) alternative (H₁) hypotheses:H₀: μ₁ = μ₂ (significant difference means two groups.)\nH₁: μ₁ ≠ μ₂ (significant difference means two groups.)\ntest statistic independent samples t-test t-value, calculated using following formula:t = (M₁ - M₂) / sqrt((s₁²/n₁) + (s₂²/n₂)):M₁ M₂ means two groups\ns₁² s₂² variances two groups\nn₁ n₂ sample sizes two groups\nt-value follows t-distribution degrees freedom (df) approximated following formula:df = min(n₁ - 1, n₂ - 1)t-value degrees freedom calculated, p-value can determined comparing t-value t-distribution appropriate degrees freedom. p-value less chosen significance level (e.g., 0.05), null hypothesis can rejected, indicating significant difference means two groups.","code":""},{"path":"independent-samples-t---test.html","id":"independent-t-test-using-r","chapter":"11 Independent Samples T - Test","heading":"11.1.1 Independent t-test using R","text":"need data two independent groups, typically stored data frame one variable representing group membership another variable representing outcome interest.Perform independent samples t-test: Use t.test() function R, specifying formula data frame arguments.output t.test() function include t-value, degrees freedom, p-value, confidence interval difference means. p-value less chosen significance level (e.g., 0.05), can reject null hypothesis, concluding significant difference means two groups.","code":"\n# Example data\ngroup <- c(\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\")\noutcome <- c(10, 12, 14, 16, 18, 20, 22, 24, 26, 28)\n\n# Create a data frame\ndata <- data.frame(group, outcome)\n# Perform the independent samples t-test\nt_test_result <- t.test(outcome ~ group, data = data)\n\n# Print the test result\nprint(t_test_result)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  outcome by group\n#> t = -5, df = 8, p-value = 0.001053\n#> alternative hypothesis: true difference in means between group A and group B is not equal to 0\n#> 95 percent confidence interval:\n#>  -14.612008  -5.387992\n#> sample estimates:\n#> mean in group A mean in group B \n#>              14              24"},{"path":"paired-t-test.html","id":"paired-t-test","chapter":"12 Paired t-test","heading":"12 Paired t-test","text":"paired samples t-test used observations collected individuals matched pairs (e.g., siblings, twins) different conditions different time points. purpose test determine significant difference means paired differences.paired t-test based following null (H₀) alternative (H₁) hypotheses:H₀: μ_d = 0 (significant difference means paired differences.)H₀: μ_d = 0 (significant difference means paired differences.)H₁: μ_d ≠ 0 (significant difference means paired differences.)H₁: μ_d ≠ 0 (significant difference means paired differences.)Mathematically, test statistic paired t-test (t-value) calculated using following formula:t = (M_d - μ_d) / (s_d / sqrt(n)):M_d mean paired differencesM_d mean paired differencesμ_d population mean difference (0 null hypothesis)μ_d population mean difference (0 null hypothesis)s_d standard deviation paired differencess_d standard deviation paired differencesn number pairsn number pairsThe t-value follows t-distribution degrees freedom (df) equal n - 1.t-value follows t-distribution degrees freedom (df) equal n - 1.paired t-test used observations within pair related matched (e.g., pre-test post-test scores individuals, scores matched pairs like siblings twins).test compares means paired differences rather means original observations.Examples:Comparing pre-test post-test scores students determine effectiveness teaching intervention.Comparing pre-test post-test scores students determine effectiveness teaching intervention.Comparing performance students two different courses taught instructor.Comparing performance students two different courses taught instructor.Recommendations:Use paired t-test dependent samples interested difference means paired differences.Use paired t-test dependent samples interested difference means paired differences.Ensure assumptions paired t-test met (see ).Ensure assumptions paired t-test met (see ).Sample size:sample size large enough provide adequate statistical power detect meaningful effect.sample size large enough provide adequate statistical power detect meaningful effect.required sample size depends effect size, significance level, desired power. can calculated using power analysis techniques (e.g., using pwr package R).required sample size depends effect size, significance level, desired power. can calculated using power analysis techniques (e.g., using pwr package R).Assumptions:Paired observations: observations within pair related matched, pre-test post-test scores individual, scores matched pairs like siblings twins.Paired observations: observations within pair related matched, pre-test post-test scores individual, scores matched pairs like siblings twins.Random sampling: pairs observations obtained random sampling population interest. ensures sample representative population results can generalized.Random sampling: pairs observations obtained random sampling population interest. ensures sample representative population results can generalized.Normality differences: differences paired observations approximately normally distributed. assumption can checked using variety methods, histograms, Q-Q plots, statistical tests like Shapiro-Wilk test.Normality differences: differences paired observations approximately normally distributed. assumption can checked using variety methods, histograms, Q-Q plots, statistical tests like Shapiro-Wilk test.Independence pairs: pairs observations independent . words, one pair’s difference influence another pair’s difference.Independence pairs: pairs observations independent . words, one pair’s difference influence another pair’s difference.Interval ratio scale data: data paired observations measured interval ratio scale. means data meaningful zero point equal intervals adjacent values.\nessential ensure assumptions met conducting paired t-test, violation assumptions may lead incorrect inferences. assumptions violated, alternative statistical tests data transformation methods might appropriate.Interval ratio scale data: data paired observations measured interval ratio scale. means data meaningful zero point equal intervals adjacent values.essential ensure assumptions met conducting paired t-test, violation assumptions may lead incorrect inferences. assumptions violated, alternative statistical tests data transformation methods might appropriate.","code":""},{"path":"paired-t-test.html","id":"performing-the-paired-t-test-using-r","chapter":"12 Paired t-test","heading":"12.0.1 Performing the paired t-test using R","text":"need data two related groups samples, typically stored data frame two variables representing paired observations. loading requirec libraries. using pysch package.","code":"\nswimdata <- read.csv(\"exampledata/Ch7_swim.csv\")\nsummary(swimdata)\n#>     pretest         posttest    \n#>  Min.   :58.00   Min.   :54.00  \n#>  1st Qu.:61.25   1st Qu.:56.25  \n#>  Median :63.50   Median :59.50  \n#>  Mean   :64.00   Mean   :59.00  \n#>  3rd Qu.:65.75   3rd Qu.:61.75  \n#>  Max.   :72.00   Max.   :64.00"},{"path":"paired-t-test.html","id":"data-screening-procedures","chapter":"12 Paired t-test","heading":"12.0.2 Data screening procedures","text":"performing paired t-test, essential check assumptions mentioned earlier, including normality differences presence outliers.Check outliers using boxplots:Check normality differences using histograms Q-Q plots:can also assess normality using Shapiro-Wilk test.","code":"\nboxplot(swimdata$pretest, swimdata$posttest, names = c(\"Pretest\", \"Posttest\"), ylab = \"Scores\", main = \"Boxplots for Pretest and Posttest\")\n# Compute the differences between paired observations\nswimdata$difference <- swimdata$posttest - swimdata$pretest\n\n# Create a histogram of the differences\nhist(swimdata$difference, main = \"Histogram of Paired Differences\", xlab = \"Difference\", ylab = \"Frequency\")\n\n# Create a Q-Q plot of the differences\nqqnorm(swimdata$difference, main = \"Q-Q Plot of Paired Differences\")\nqqline(swimdata$difference, col = \"red\")\nshapiro.test(swimdata$difference)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  swimdata$difference\n#> W = 0.95557, p-value = 0.7344"},{"path":"paired-t-test.html","id":"performing-the-paired-t-test","chapter":"12 Paired t-test","heading":"12.0.3 Performing the paired t-test","text":"Paired t-test can performed using t.test() function.output paired t-test includes several pieces information:t-value: calculated t-statistic 7.3193. value represents difference means pretest posttest scores, terms standard deviations.Degrees freedom (df): degrees freedom t-test 9, calculated number pairs minus 1 (n - 1).p-value: p-value 4.472e-05 (0.00004472), probability observing t-value extreme extreme calculated t-value, assuming null hypothesis (significant difference means paired differences) true.Alternative hypothesis: output states alternative hypothesis true mean difference equal 0.95% confidence interval: confidence interval [3.454652, 6.545348], means can 95% confident true population mean difference lies within interval.Sample estimates: mean difference pretest posttest scores 5.Based output, since p-value (0.00004472) less common significance level (0.05), can reject null hypothesis conclude significant difference pretest posttest scores swimdata dataset. positive mean difference (5) indicates , average, posttest scores higher pretest scores. 95% confidence interval suggests true population mean difference lies 3.454652 6.545348.assumptions paired t-test met p-value less chosen significance level, can conclude significant difference means paired differences. Otherwise, assumptions violated, consider alternative statistical tests data transformation methods.","code":"\n# Perform the paired t-test\nt_test_result <- t.test(swimdata$pretest, swimdata$posttest, paired = TRUE)\n\n# Print the test result\nprint(t_test_result)\n#> \n#>  Paired t-test\n#> \n#> data:  swimdata$pretest and swimdata$posttest\n#> t = 7.3193, df = 9, p-value = 4.472e-05\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  3.454652 6.545348\n#> sample estimates:\n#> mean difference \n#>               5"},{"path":"covariance.html","id":"covariance","chapter":"13 Covariance","heading":"13 Covariance","text":"Covariation, covariance, measure two variables change together. values one variable increase values variable increase, covariance positive, indicating positive relationship. values one variable decrease values variable increase, covariance negative, indicating negative relationship. apparent pattern variables, covariance close 0, indicating relationship.Mathematically, covariance two variables X Y, n observations , can calculated using formula:Cov(X, Y) = Σ((Xi - X_mean) * (Yi - Y_mean)) / (n - 1):Xi Yi individual data points X_mean Y_mean means X Y variables, respectively Σ denotes sum data points n number data points scatterplot, covariance can visually inferred direction closeness points. points tightly clustered around positive slope, covariance positive. clustered around negative slope, covariance negative. points scattered randomly clear pattern, covariance close 0.can use R inbuilt cov() function calculate covariance.covariance 4.5 means positive relationship two variables. one variable increases, variable also tends increase. However, important note covariance alone provide information strength relationship, scale covariance depends scales two variables.better understand strength direction relationship two variables, can calculate correlation coefficient (e.g., Pearson’s correlation coefficient), standardized measure association ranges -1 1.","code":"\nlibrary(readr)\n#> Warning: package 'readr' was built under R version 4.2.2\n# Read the CSV file into a data frame\ncovdata <- read_csv(\"exampledata/Ch10_kidspets.csv\")\n#> Rows: 5 Columns: 2\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl (2): Children, Pets\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate the covariance between two variables (e.g., var1 and var2)\ncovariance <- cov(covdata$Children, covdata$Pets)\nprint(covariance)\n#> [1] 4.5"},{"path":"scatter-plots-1.html","id":"scatter-plots-1","chapter":"14 Scatter Plots","heading":"14 Scatter Plots","text":"","code":""},{"path":"scatter-plots-1.html","id":"bivariate-measures-of-association","chapter":"14 Scatter Plots","heading":"14.1 Bivariate Measures of Association","text":"Bivariate measures association statistical methods used examine strength, direction, nature relationship two variables. educational research, measures help researchers understand associations different factors may influence educational outcomes, student performance, teaching strategies, resource allocation. understanding relationships, researchers can make informed decisions develop effective interventions improve educational outcomes.","code":""},{"path":"scatter-plots-1.html","id":"scatterplots","chapter":"14 Scatter Plots","heading":"14.2 Scatterplots","text":"Scatterplots important tool understanding bivariate measures association. scatterplot graphical representation relationship two variables, point plot represents pair observations two variables. pattern points can give us idea direction, strength, shape relationship two variables.example, educational research, scatterplot used visualize relationship students’ reading scores math scores. examining scatterplot, researchers can identify whether positive negative relationship two variables, whether relationship linear nonlinear, strong association .key insights scatterplots can provide:Direction: direction relationship two variables can positive, negative, relationship. positive relationship, one variable increases, variable also increases. negative relationship, one variable increases, variable decreases. relationship, points scattered randomly, indicating association two variables.Strength: strength relationship can determined closely points follow specific pattern (e.g., straight line). strong relationship points closely following pattern, weak relationship points scattered widely around pattern.Shape: shape relationship can linear, nonlinear, relationship. linear relationship follows straight line, nonlinear relationship follows curve non-straight pattern. relationship means points scattered randomly, indicating association two variables.example, lets create five different types plots see differ terms attributes.R code generate 5 different possible scatterplots, representing different type relationship. Lets display individually see show relationship.","code":"\n# Load ggplot2 package\nlibrary(ggplot2)\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n\n# Create sample datasets\nset.seed(42)\n\npositive_linear <- data.frame(x = 1:50, y = 1:50 + rnorm(50, sd = 5))\nnegative_linear <- data.frame(x = 1:50, y = 50:1 + rnorm(50, sd = 5))\nnonlinear <- data.frame(x = 1:50, y = (1:50)^2 + rnorm(50, sd = 500))\nno_relationship <- data.frame(x = 1:50, y = rnorm(50))\nclustered <- data.frame(x = c(rnorm(25, mean = 20), rnorm(25, mean = 40)), y = c(rnorm(25, mean = 30), rnorm(25, mean = 50)))\n\n# Function to create scatterplots\ncreate_scatterplot <- function(data, title) {\n  ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    xlab(\"X\") +\n    ylab(\"Y\") +\n    ggtitle(title) +\n    theme_minimal()\n}\n\n# Generate scatterplots\npositive_linear_plot <- create_scatterplot(positive_linear, \"Positive Linear Relationship\")\nnegative_linear_plot <- create_scatterplot(negative_linear, \"Negative Linear Relationship\")\nnonlinear_plot <- create_scatterplot(nonlinear, \"Nonlinear Relationship\")\nno_relationship_plot <- create_scatterplot(no_relationship, \"No Relationship\")\nclustered_plot <- create_scatterplot(clustered, \"Clustered Relationship\")"},{"path":"scatter-plots-1.html","id":"positive-linear-relationship","chapter":"14 Scatter Plots","heading":"14.2.0.1 Positive linear relationship:","text":"points scatterplot show upward trend, indicating positive relationship variables.","code":"\nprint(positive_linear_plot)"},{"path":"scatter-plots-1.html","id":"negative-linear-relationship","chapter":"14 Scatter Plots","heading":"14.2.0.2 Negative linear relationship","text":"points scatterplot show downward trend, indicating negative relationship variables.","code":"\nprint(negative_linear_plot)"},{"path":"scatter-plots-1.html","id":"nonlinear-relationship","chapter":"14 Scatter Plots","heading":"14.2.0.3 Nonlinear relationship","text":"points scatterplot follow curve non-straight pattern, indicating nonlinear relationship variables.","code":"\nprint(nonlinear_plot)"},{"path":"scatter-plots-1.html","id":"no-relationship","chapter":"14 Scatter Plots","heading":"14.2.0.4 No relationship","text":"points scatterplot scattered randomly, indicating association variables.","code":"\nprint(no_relationship_plot)"},{"path":"scatter-plots-1.html","id":"clustered-relationship","chapter":"14 Scatter Plots","heading":"14.2.0.5 Clustered relationship","text":"points scatterplot form clusters, indicating relationship variables may complex may factors play.scatterplots can help visualize understand associations different variables data.","code":"\nprint(clustered_plot)"},{"path":"scatter-plots-1.html","id":"generating-scatterplots-using-r","chapter":"14 Scatter Plots","heading":"14.3 Generating Scatterplots using R","text":"First, let’s install load ggplot2 package, create sample dataset:Now, can create scatterplot reading_scores vs. math_scores.example , used ggplot2 package create scatterplot. aes function maps x y axes reading_scores math_scores variables, respectively. geom_point function adds points scatterplot, representing paired observations reading_scores math_scores. xlab, ylab, ggtitle, theme_minimal functions used customize appearance scatterplot.","code":"\n# Load ggplot2 package\nlibrary(ggplot2)\n\n# Create a sample dataset\ndata <- data.frame(\n  reading_scores = c(50, 60, 65, 55, 70, 75, 80, 85, 90, 95),\n  math_scores = c(55, 60, 70, 50, 75, 80, 85, 90, 95, 100)\n)\n# Create a scatterplot\nscatterplot <- ggplot(data, aes(x = reading_scores, y = math_scores)) +\n  geom_point() +\n  xlab(\"Reading Scores\") +\n  ylab(\"Math Scores\") +\n  ggtitle(\"Scatterplot of Reading Scores vs. Math Scores\") +\n  theme_minimal()\n\n# Display the scatterplot\nprint(scatterplot)"},{"path":"correlation.html","id":"correlation","chapter":"15 Correlation","heading":"15 Correlation","text":"Correlation standardized measure linear relationship two variables. Pearson’s correlation coefficient (r), commonly used correlation measure, ranges -1 1, -1 indicating perfect negative relationship, 1 indicating perfect positive relationship, 0 indicating linear relationship.Mathematically, Pearson’s correlation coefficient (r) can calculated using following formula:r = Σ((Xi - X_mean) * (Yi - Y_mean)) / (sqrt(Σ(Xi - X_mean)^2) * sqrt(Σ(Yi - Y_mean)^2)):Xi Yi individual data points variables X Y, respectively X_mean Y_mean means variables X Y, respectively Σ denotes sum data points formula calculates correlation coefficient dividing covariance X Y product standard deviations.interpret result:correlation coefficient close 1 indicates strong positive relationship variables, meaning one variable increases, variable also tends increase.correlation coefficient close -1 indicates strong negative relationship, meaning one variable increases, variable tends decrease.correlation coefficient close 0 suggests linear relationship two variables.significant correlation two variables indicates relationship , necessarily mean one variable causes . Additional research analysis may needed establish causality.","code":"\nlibrary(readr)\n#> Warning: package 'readr' was built under R version 4.2.2\n# Read the CSV file into a data frame\ncordata <- read_csv(\"exampledata/Ch10_kidspets.csv\")\n#> Rows: 5 Columns: 2\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl (2): Children, Pets\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Calculate the covariance between two variables (e.g., var1 and var2)\ncorrelation<- cor(cordata$Children, cordata$Pets)\nprint(correlation)\n#> [1] 0.9"},{"path":"simple-linear-regression.html","id":"simple-linear-regression","chapter":"16 Simple Linear Regression","heading":"16 Simple Linear Regression","text":"Simple linear regression statistical method used model \nrelationship single independent variable (predictor) \ndependent variable (outcome). basic technique can help\nresearchers understand association two continuous variables\nmake predictions based observed data.simple linear regression, try find best-fitting straight\nline data points scatterplot. line represents \npredicted value dependent variable (Y) given value \nindependent variable (X). equation line follows:Y = b0 + b1X + εHere:Y dependent variable (outcome) X independent variable\n(predictor) b0 intercept, represents value Y X\nzero b1 slope, represents change Y one-unit\nincrease X ε error term, accounts difference\nactual predicted values Y simple linear\nregression, goal find values b0 b1 minimize\nsum squared differences observed values Y \npredicted values (based line). method called \nleast squares estimation.better understand simple linear regression, let’s consider \nscatterplot two variables X Y:","code":"y\n|\n|       •\n|     •\n|   •\n| •\n+----------------\n  x"},{"path":"simple-linear-regression.html","id":"regression-using-r","chapter":"16 Simple Linear Regression","heading":"16.1 Regression using R","text":"perform simple linear regression R using psych package, can\nuse following code:output summary simple linear regression model fitted \ndata, Success dependent variable (outcome), represents score employment success scale.\nOptimism independent variable (predictor),represents scores work optimism scale.Let’s break piece piece:Residuals: Residuals differences observed \npredicted values dependent variable. summary provides \nminimum, 1st quartile, median, 3rd quartile, maximum residuals. \ninformation helps us understand spread residuals, \nideally evenly distributed close zero.Coefficients: coefficients table provides estimates,\nstandard errors, t-values, p-values intercept predictor\nvariable (Optimism).estimated intercept (b₀) 8.86473, estimated slope (b₁)\nOptimism predictor 0.52496. means linear\nregression equation :Y = 8.86473 + 0.52496 * OptimismThe p-value intercept 0.088358, greater 0.05,\nsuggesting intercept statistically significant \n0.05 level. p-value Optimism predictor 0.000181, \nless 0.001, indicating Optimism statistically significant\n0.001 level.Model fit diagnostics: output provides information \nmodel’s goodness fit diagnostic statistics.","code":"\n# Load required packages\nlibrary(readr)\n#> Warning: package 'readr' was built under R version 4.2.2\nlibrary(psych)\n#> Warning: package 'psych' was built under R version 4.2.3\n\n# Read the CSV file into a data frame\ndata_file <- \"exampledata/EmpSuccess.csv\"\ndata <- read_csv(data_file)\n#> Rows: 10 Columns: 2\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl (2): Optimism, Success\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Perform simple linear regression\nmodel <- lm(Success ~ Optimism, data = data)\n\n# Display the model summary\nsummary(model)\n#> \n#> Call:\n#> lm(formula = Success ~ Optimism, data = data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.4380 -1.9932  0.1626  2.2568  3.7118 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  8.86473    4.56965   1.940 0.088358 .  \n#> Optimism     0.52496    0.08034   6.535 0.000181 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.165 on 8 degrees of freedom\n#> Multiple R-squared:  0.8422, Adjusted R-squared:  0.8225 \n#> F-statistic:  42.7 on 1 and 8 DF,  p-value: 0.0001814"},{"path":"simple-linear-regression.html","id":"scatter-plot","chapter":"16 Simple Linear Regression","heading":"16.2 Scatter Plot","text":"can create scatter plot data add regression line using ggplot2 package:","code":"\n# Load required packages\nlibrary(ggplot2)\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> \n#> Attaching package: 'ggplot2'\n#> The following objects are masked from 'package:psych':\n#> \n#>     %+%, alpha\n\n# Create a scatterplot with the regression line\nggplot(data, aes(x = Optimism, y = Success)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  xlab(\"Optimism\") +\n  ylab(\"Success\") +\n  ggtitle(\"Simple Linear Regression\") +\n  theme_minimal()\n#> `geom_smooth()` using formula = 'y ~ x'"},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"17 Multiple Regression","heading":"17 Multiple Regression","text":"Multiple linear regression extension simple linear regression, model relationship single dependent (outcome) variable multiple independent (predictor) variables. goal predict dependent variable based values independent variables accounting influence predictor variable.multiple linear regression equation follows:Y = β0 + β1X1 + β2X2 + … + βnXn + εWhere:Y dependent variable\nβ0 intercept (value Y independent variables zero)\nβ1, β2, …, βn regression coefficients independent variable X1, X2, …, Xn\nε error term, representing difference actual predicted values Y\nregression coefficients (β1, β2, …, βn) represent average change dependent variable one-unit increase corresponding independent variable, holding independent variables constant.can perform linear regression R using psych package.output shows us regression coefficients, standard errors, t-values, p-values independent variable. can also see R-squared value, adjusted R-squared value, model fit statistics.summary statistics residuals provided, including minimum, first quartile (1Q), median, third quartile (3Q), maximum values.coefficients table provides regression coefficients (Estimate), standard errors (Std. Error), t-values (t value), p-values (Pr(>|t|)) independent variable, well intercept.regression equation based output :Y = 0.637906 + 0.468670 * UGPA + 0.012463 * GRE_TotalWhere:Y dependent variable\nUGPA first independent variable (Undergraduate GPA)\nGRE_Total second independent variable (Total GRE score)\nSignificance codes: indicate level statistical significance independent variable, 0.001 (*), 0.01 (), 0.05 (.), 0.1 ( ).","code":"\n# Load the necessary libraries.\nlibrary(psych)\n#> Warning: package 'psych' was built under R version 4.2.3\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version\n#> 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.2\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.2\n#> Warning: package 'lubridate' was built under R version\n#> 4.2.3\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.1     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ ggplot2::%+%()   masks psych::%+%()\n#> ✖ ggplot2::alpha() masks psych::alpha()\n#> ✖ dplyr::filter()  masks stats::filter()\n#> ✖ dplyr::lag()     masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n# Reading the CSV file. \ndata <- read_csv(\"exampledata/Ch18_GGPA.csv\")\n#> Rows: 11 Columns: 3\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl (3): GRE_Total, UGPA, GGPA\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Define the model. \nmodel <- lm(GGPA  ~UGPA + GRE_Total, data = data)\n\nsummary(model)\n#> \n#> Call:\n#> lm(formula = GGPA ~ UGPA + GRE_Total, data = data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.19943 -0.06029  0.02812  0.06216  0.17207 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.637906   0.326537   1.954 0.086517 .  \n#> UGPA        0.468670   0.093181   5.030 0.001015 ** \n#> GRE_Total   0.012463   0.002288   5.447 0.000611 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1127 on 8 degrees of freedom\n#> Multiple R-squared:  0.9076, Adjusted R-squared:  0.8845 \n#> F-statistic: 39.29 on 2 and 8 DF,  p-value: 7.289e-05"},{"path":"multiple-regression.html","id":"model-fit-statistics","chapter":"17 Multiple Regression","heading":"17.0.1 Model Fit Statistics:","text":"Residual standard error: standard deviation residuals, indicating average difference actual predicted values dependent variable.Multiple R-squared: value represents proportion total variability dependent variable explained independent variables model.Adjusted R-squared: value adjusts R-squared number independent variables, providing accurate estimate model’s explanatory power multiple predictors included.F-statistic p-value: values indicate overall significance regression model, F-statistic measuring ratio explained variance unexplained variance p-value providing probability observing F-statistic null hypothesis (.e., regression coefficients equal zero).","code":""},{"path":"one-way-anova.html","id":"one-way-anova","chapter":"18 One way ANOVA","heading":"18 One way ANOVA","text":"One-way Analysis Variance (ANOVA) statistical method used analyze differences means three groups. fixed-effects model, means assumes levels independent variable (factor) fixed specifically chosen researcher. educational research, one-way ANOVA can used compare mean scores students different schools, teaching methods, grade levels.basic idea behind one-way ANOVA partition total variation dependent variable two components: -group variation within-group variation. -group variation represents differences means groups, within-group variation represents differences within group can attributed random error individual differences.Mathematically, one-way ANOVA model can represented :Y_ij = μ + α_i + ε_ijWhere:Y_ij observation j-th individual -th group\nμ overall mean dependent variable\nα_i effect -th group (difference group mean overall mean)\nε_ij error term (residual difference observed value predicted value)","code":""},{"path":"one-way-anova.html","id":"characteristics","chapter":"18 One way ANOVA","heading":"18.1 Characteristics","text":"characteristics one-way ANOVA include:One independent variable (factor) least three levels (groups): One-way ANOVA designed analyze differences means three groups. independent variable categorical, level represents distinct category.Independent observations: observations group independent , means outcome one individual influence outcome another individual.Normality: dependent variable approximately normally distributed within group. assumption can checked using graphical techniques (e.g., histograms Q-Q plots) formal tests (e.g., Shapiro-Wilk test).Homogeneity variances: variances dependent variable approximately equal across groups. assumption can checked using Levene’s test Bartlett’s test.Interval ratio scale measurement: dependent variable measured interval ratio scale, means differences values meaning, true zero point.One-way ANOVA powerful technique comparing means multiple groups, allowing researchers determine whether statistically significant differences .","code":""},{"path":"one-way-anova.html","id":"performing-one-way-anova-using-r","chapter":"18 One way ANOVA","heading":"18.2 Performing one Way ANOVA using R","text":"base R aov() function car package used actual ANOVA homogeneity tests.Lets break output:\nLet’s break :Df (Degrees Freedom): degrees freedom “Sport” factor 1, “Residuals” (error term) 30. degrees freedom used calculation F value.Sum Sq (Sum Squares): sum squares “Sport” factor 709.8, “Residuals” 1039.9. sum squares represents variation data accounted factor (Sport) error term (Residuals).Mean Sq (Mean Squares): mean squares “Sport” factor 709.8, “Residuals” 34.7. Mean squares calculated dividing sum squares degrees freedom.F value: F value 20.48. test statistic one-way ANOVA, calculated dividing mean square “Sport” factor mean square “Residuals”. F value used determine statistically significant difference group means.p-value: p-value “Sport” factor 8.87e-05 (0.0000887). probability observing F value extreme one calculated (20.48) true differences group means (null hypothesis).\nSince p-value (0.0000887) less significance level (0.05), can conclude statistically significant difference group means “Sport” factor(choice participation).can also check homogeneity, normality effect size.Shapiro-Wilk test used assess whether sample data comes normally distributed population.four sports, p-values four sports greater significance level 0.05, indicating reject null hypothesis data normally distributed. suggests ‘Distress’ variable within sport group likely normally distributed, important assumption parametric statistical tests one-way ANOVA.Levene’s test used assess variances different groups equal, important assumption one-way ANOVA. p-value Levene’s test indicates assumption homogeneity variances met data. supports use one-way ANOVA analyze differences group means.","code":"\n# Load required libraries\nlibrary(psych)\n#> Warning: package 'psych' was built under R version 4.2.3\n\n# Read the CSV file\nourdata <- read.csv(\"exampledata/Ch11_distress.csv\")\n\n# Inspect the data\nhead(ourdata)\n#>   Sport Distress\n#> 1     1       15\n#> 2     1       10\n#> 3     1       12\n#> 4     1        8\n#> 5     1       21\n#> 6     1        7\n\n#Summary Statistics\nsummary(ourdata)\n#>      Sport         Distress    \n#>  Min.   :1.00   Min.   : 3.00  \n#>  1st Qu.:1.75   1st Qu.:12.00  \n#>  Median :2.50   Median :20.00  \n#>  Mean   :2.50   Mean   :18.41  \n#>  3rd Qu.:3.25   3rd Qu.:25.00  \n#>  Max.   :4.00   Max.   :30.00\n\n# Perform one-way ANOVA\nanova_result <- aov(Distress ~ Sport, data = ourdata)\n\n# Display the result\nsummary(anova_result)\n#>             Df Sum Sq Mean Sq F value   Pr(>F)    \n#> Sport        1  709.8   709.8   20.48 8.87e-05 ***\n#> Residuals   30 1039.9    34.7                     \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Perform Shapiro-Wilk test for normality for each group\nshapiro_test1 <- shapiro.test(ourdata$Distress[ourdata$Sport==\"1\"])\nshapiro_test2 <- shapiro.test(ourdata$Distress[ourdata$Sport==\"2\"])\nshapiro_test3 <- shapiro.test(ourdata$Distress[ourdata$Sport==\"3\"])\nshapiro_test4 <- shapiro.test(ourdata$Distress[ourdata$Sport==\"4\"])\n\n# Display the results\nshapiro_test1\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  ourdata$Distress[ourdata$Sport == \"1\"]\n#> W = 0.9847, p-value = 0.9823\nshapiro_test2\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  ourdata$Distress[ourdata$Sport == \"2\"]\n#> W = 0.93162, p-value = 0.531\nshapiro_test3\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  ourdata$Distress[ourdata$Sport == \"3\"]\n#> W = 0.90504, p-value = 0.3204\nshapiro_test4\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  ourdata$Distress[ourdata$Sport == \"4\"]\n#> W = 0.93347, p-value = 0.5482\n\n# Load car library for Levene's test\nlibrary(car)\n#> Warning: package 'car' was built under R version 4.2.3\n#> Loading required package: carData\n#> Warning: package 'carData' was built under R version 4.2.3\n#> \n#> Attaching package: 'car'\n#> The following object is masked from 'package:psych':\n#> \n#>     logit\n\n# Perform Levene's test\nlevene_test <- leveneTest(ourdata$Distress, ourdata$Sport )\n#> Warning in leveneTest.default(ourdata$Distress,\n#> ourdata$Sport): ourdata$Sport coerced to factor.\n\n# Display the result\nlevene_test\n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value Pr(>F)\n#> group  3  0.6039  0.618\n#>       28"},{"path":"post-hoc-tests.html","id":"post-hoc-tests","chapter":"19 Post hoc tests","heading":"19 Post hoc tests","text":"Post hoc tests statistical analyses performed ANOVA omnibus tests determine specific group means significantly different . tests necessary ANOVA tells ’s significant difference groups doesn’t identify groups different.","code":""},{"path":"post-hoc-tests.html","id":"tukeys-post-hoc-tests","chapter":"19 Post hoc tests","heading":"19.1 Tukey’s Post Hoc Tests","text":"Tukey’s post hoc test, also known Tukey’s Honestly Significant Difference (HSD) test, widely used method performing multiple comparisons. important controls family-wise error rate (FWER), probability making least one Type error (false positive) performing multiple tests. controlling FWER, Tukey’s HSD test helps reduce risk identifying false significant differences groups due chance alone.","code":""},{"path":"post-hoc-tests.html","id":"performing-tukeys-post-hoc-tests-using-r","chapter":"19 Post hoc tests","heading":"19.2 Performing Tukey’s Post Hoc Tests using R","text":"running one-way ANOVA using aov() function, shown previous answer, can use multcomp package perform Tukey’s post hoc test.create new variable dataframe named “Factors”define Sport variable nominal giving cetain names.generate one way ANOVA using new factor varaible.post hoc test,first part output shows estimated mean differences groups, standard errors, t-values, adjusted p-values. second part output presents confidence intervals pairwise comparisons.’s breakdown output:Estimate: estimated mean difference pair groups.\nStd. Error: standard error associated estimate.\nt value: t-statistic pairwise comparison.\nPr(>|t|): adjusted p-value pairwise comparison, controlling multiple comparisons (single-step method).case, fielding vs. movement comparison territory vs. movement comparison significant 0.05 0.001 levels, respectively.second part output provides simultaneous confidence intervals pairwise comparisons. ‘lwr’ ‘upr’ columns represent lower upper bounds 95% confidence intervals, respectively. confidence interval include zero, indicates significant difference corresponding pair group means.confidence intervals, can see fielding vs. movement territory vs. movement comparisons intervals include zero, consistent significant adjusted p-values found first part output.","code":"\n\n# Read the CSV file\nourdata <- read.csv(\"exampledata/Ch11_distress.csv\")\n\n# Inspect the data\nhead(ourdata)\n#>   Sport Distress\n#> 1     1       15\n#> 2     1       10\n#> 3     1       12\n#> 4     1        8\n#> 5     1       21\n#> 6     1        7\n\n# Creating new variable as factors\nourdata$SportF <- factor(ourdata$Sport, labels= c(\"movement\", \"target\", \"fielding\", \"territory\"))\n\n#Always good idea to verify. \nsummary(ourdata)\n#>      Sport         Distress           SportF \n#>  Min.   :1.00   Min.   : 3.00   movement :8  \n#>  1st Qu.:1.75   1st Qu.:12.00   target   :8  \n#>  Median :2.50   Median :20.00   fielding :8  \n#>  Mean   :2.50   Mean   :18.41   territory:8  \n#>  3rd Qu.:3.25   3rd Qu.:25.00                \n#>  Max.   :4.00   Max.   :30.00\n# Perform one-way ANOVA\nanova_result <- aov(Distress ~ SportF, data = ourdata)\n\n# Display the result\nsummary(anova_result)\n#>             Df Sum Sq Mean Sq F value  Pr(>F)   \n#> SportF       3  738.6  246.20   6.818 0.00136 **\n#> Residuals   28 1011.1   36.11                   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Load the necessary packages\nlibrary(multcomp)\n#> Warning: package 'multcomp' was built under R version 4.2.3\n#> Loading required package: mvtnorm\n#> Loading required package: survival\n#> Loading required package: TH.data\n#> Warning: package 'TH.data' was built under R version 4.2.3\n#> Loading required package: MASS\n#> \n#> Attaching package: 'TH.data'\n#> The following object is masked from 'package:MASS':\n#> \n#>     geyser\n\n# Run Tukey's HSD post hoc test\ntukey_results <- glht(anova_result, linfct = mcp(SportF = \"Tukey\"))\n\n# Summary of Tukey's HSD test\nsummary(tukey_results)\n#> \n#>   Simultaneous Tests for General Linear Hypotheses\n#> \n#> Multiple Comparisons of Means: Tukey Contrasts\n#> \n#> \n#> Fit: aov(formula = Distress ~ SportF, data = ourdata)\n#> \n#> Linear Hypotheses:\n#>                           Estimate Std. Error t value\n#> target - movement == 0       6.750      3.005   2.247\n#> fielding - movement == 0     9.125      3.005   3.037\n#> territory - movement == 0   13.250      3.005   4.410\n#> fielding - target == 0       2.375      3.005   0.790\n#> territory - target == 0      6.500      3.005   2.163\n#> territory - fielding == 0    4.125      3.005   1.373\n#>                           Pr(>|t|)    \n#> target - movement == 0      0.1354    \n#> fielding - movement == 0    0.0247 *  \n#> territory - movement == 0   <0.001 ***\n#> fielding - target == 0      0.8582    \n#> territory - target == 0     0.1582    \n#> territory - fielding == 0   0.5261    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> (Adjusted p values reported -- single-step method)\n\n# Confidence intervals for the pairwise comparisons\nconfint(tukey_results)\n#> \n#>   Simultaneous Confidence Intervals\n#> \n#> Multiple Comparisons of Means: Tukey Contrasts\n#> \n#> \n#> Fit: aov(formula = Distress ~ SportF, data = ourdata)\n#> \n#> Quantile = 2.7298\n#> 95% family-wise confidence level\n#>  \n#> \n#> Linear Hypotheses:\n#>                           Estimate lwr     upr    \n#> target - movement == 0     6.7500  -1.4520 14.9520\n#> fielding - movement == 0   9.1250   0.9230 17.3270\n#> territory - movement == 0 13.2500   5.0480 21.4520\n#> fielding - target == 0     2.3750  -5.8270 10.5770\n#> territory - target == 0    6.5000  -1.7020 14.7020\n#> territory - fielding == 0  4.1250  -4.0770 12.3270"},{"path":"chi-square-tests.html","id":"chi-square-tests","chapter":"20 Chi Square Tests","heading":"20 Chi Square Tests","text":"Chi-square tests non-parametric statistical tests used determine whether significant relationship variables observed frequencies sample consistent expected frequencies. two main types chi-square tests: chi-square goodness fit test chi-square test independence (association).","code":""},{"path":"chi-square-tests.html","id":"chi-square-goodness-of-fit-test","chapter":"20 Chi Square Tests","heading":"20.1 Chi Square Goodness of Fit Test","text":"test used determine whether observed frequencies sample match expected frequencies based specified distribution. univariate test, meaning involves one categorical variable. null hypothesis goodness fit test observed frequencies consistent expected frequencies.test statistic (χ²) calculated follows:χ² = Σ [(O_i - E_i)² / E_i]O_i observed frequencies, E_i expected frequencies, Σ represents summation across categories.calculated χ² value large, indicates significant difference observed expected frequencies, reject null hypothesis.","code":""},{"path":"chi-square-tests.html","id":"performing-chi-square-goodness-of-fit","chapter":"20 Chi Square Tests","heading":"20.1.1 Performing Chi Square Goodness of Fit","text":"Let’s take simple example first.\nSuppose school 200 students, know distribution students’ grades , B, C, D follow specific proportion: 25% , 35% B, 25% C, 15% D. want know observed grade distribution school matches expected distribution.First, ’ll create table observed grade frequencies:Next, ’ll define expected proportions calculate expected frequencies:Now, ’ll perform chi-square goodness fit test using chisq.test() function. chisq.test performs chi-squared contingency table tests goodness--fit tests.Let’s break part output:X-squared = 3.4286: chi-square test statistic calculated observed frequencies expected frequencies. quantifies difference observed distribution expected distribution.df = 3: degrees freedom test, calculated number categories minus 1. case, 4 grade categories (, B, C, D), degrees freedom 4 - 1 = 3.p-value = 0.3301: p-value associated test statistic, represents probability observing chi-square test statistic extreme extreme one calculated null hypothesis true. null hypothesis states observed distribution grades follows expected distribution.Based output, p-value (0.3301) greater common significance level 0.05. Therefore, fail reject null hypothesis, conclude significant difference observed distribution grades expected distribution school. words, observed grade distribution consistent expected distribution according chi-square goodness fit test.","code":"\nobserved <- c(A = 40, B = 80, C = 50, D = 30)\nexpected_proportions <- c(A = 0.25, B = 0.35, C = 0.25, D = 0.15)\ntotal_students <- sum(observed)\nexpected <- total_students * expected_proportions\nchisq_gof <- chisq.test(observed, p = expected_proportions)\nchisq_gof#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  observed\n#> X-squared = 3.4286, df = 3, p-value = 0.3301"},{"path":"chi-square-tests.html","id":"chi-sqaure-test-of-association","chapter":"20 Chi Square Tests","heading":"20.2 Chi Sqaure test of association","text":"test used determine whether significant association two categorical variables contingency table. null hypothesis test independence two variables independent, meaning association .test statistic (χ²) calculated similarly goodness fit test, contingency table:χ² = Σ [(O_ij - E_ij)² / E_ij]O_ij observed frequencies contingency table, E_ij expected frequencies (calculated (row total × column total) / grand total), Σ represents summation across cells table.calculated χ² value large, indicates significant association two variables, reject null hypothesis.chi-square tests assumptions, including data categorical, sample random, categories mutually exclusive exhaustive. Additionally, expected frequencies sufficiently large (typically, least 5) test valid.data analysis research, tests helpful understanding whether distribution categorical variable fits particular pattern whether two categorical variables associated , can provide insights potential relationships associations data.","code":""},{"path":"chi-square-tests.html","id":"performing-chi-sqaure-test-of-independence","chapter":"20 Chi Square Tests","heading":"20.2.1 Performing Chi-sqaure test of independence","text":"Let’s say want know association students’ grades participation extracurricular activities. contingency table observed frequencies.perform chi-square test independence, ’ll use chisq.test() function:Based output, p-value (8.699e-07) much smaller common significance level 0.05. Therefore, reject null hypothesis conclude significant association two categorical variables. words, distribution grades independent groups, relationship group membership grade distribution according Pearson’s chi-square test independece.","code":"\nobserved_table <- matrix(c(20, 20, 60, 20, 30, 20, 10, 20), nrow = 2, byrow = TRUE)\ncolnames(observed_table) <- c(\"A\", \"B\", \"C\", \"D\")\nrownames(observed_table) <- c(\"Participate\", \"Not_Participate\")\nchisq_independence <- chisq.test(observed_table)\nchisq_independence\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  observed_table\n#> X-squared = 30.952, df = 3, p-value = 8.699e-07"},{"path":"g-power.html","id":"g-power","chapter":"21 G* Power","heading":"21 G* Power","text":"Power analysis essential step research planning determine probability detecting effect certain size exists. context hypothesis testing, power probability correctly rejecting null hypothesis false.G* Power statistical software tool used power analysis sample size calculation various research designs. significance G* Power educational research lies ability help researchers design plan studies effectively. conducting power analyses, researchers can:Determine appropriate sample size: G*Power allows researchers estimate minimum sample size required achieve desired level statistical power, considering effect size alpha level (significance level). helps ensure study adequately powered reduces risk Type II errors (failing reject false null hypothesis).Assess power study: G*Power can used estimate power study given sample size, effect size, alpha level. helps researchers understand likelihood detecting meaningful effect exists, aids interpretation study results.Sensitivity analysis: G*Power can also used conduct sensitivity analyses, determine smallest effect size study capable detecting adequate power. Sensitivity analysis helps researchers understand limitations study refine research questions hypotheses accordingly.","code":""},{"path":"g-power.html","id":"gpower-in-r","chapter":"21 G* Power","heading":"21.1 G*Power in R","text":"calculate G*Power using R, can use pwr package, specifically designed power analysis sample size calculation. First, ’ll need install load pwr package:’ve loaded pwr package, can use various functions perform power analysis different types tests. examples:","code":"\ninstall.packages(\"pwr\")\nlibrary(pwr)"},{"path":"g-power.html","id":"for-a-t-test-two-sample-equal-sample-sizes-and-equal-variance","chapter":"21 G* Power","heading":"21.1.1 For a t-test (two-sample, equal sample sizes, and equal variance)","text":"","code":"\npwr.t.test(n = NULL, d = effect_size, sig.level = 0.05, power = 0.8, type = \"two.sample\", alternative = \"two.sided\")"},{"path":"g-power.html","id":"for-correlation","chapter":"21 G* Power","heading":"21.1.2 For Correlation","text":"","code":"\npwr.r.test(n = NULL, r = effect_size, sig.level = 0.05, power = 0.8, alternative = \"two.sided\")"},{"path":"g-power.html","id":"for-a-chi-squared-test","chapter":"21 G* Power","heading":"21.1.3 For a chi-squared test:","text":"","code":"\npwr.chisq.test(w = effect_size, N = total_sample_size, df = degrees_of_freedom, sig.level = 0.05, power = NULL)"},{"path":"g-power.html","id":"for-a-one-way-anova","chapter":"21 G* Power","heading":"21.1.4 For a one-way ANOVA:","text":"","code":"\npwr.anova.test(k = number_of_groups, n = sample_size_per_group, f = effect_size, sig.level = 0.05, power = NULL)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
